---
layout:     post
title:      "《Python 机器学习》第4章学习总结"
date:       2019-04-14
author:     "董国珍"
header-img: "img/post-bg-2015.jpg"
tags:
    - 数据分析
---




#  <center>《Python 机器学习》第4章学习总结<center>  
   

## 第四章 惩罚线性回归模型
  

#### 惩罚方法是如何确定解的范围以及解的类型

本章介绍一族用于克服最小二乘法（OLS）过拟合问题的方法，即惩罚线性回归方法。  
  
岭回归是惩罚线性回归的一个特例，通过对回归系数的平方和进行惩罚来避免过拟合
  
* 4.1 为什么惩罚线性回归方法如此有效
  
> 有效原因：
> 1. 模型训练足够快速(足够快速地估计系数)
> > 
> > 为模型的构建往往迭代进行(模型训练是特征选择以及特征工程的基础）  
> > 
> > 条件改变的话，可能需要重新训练模型  
> >
> 2. 变量的重要性信息(包括对模型属性进行排序)  
> > 
> > 属性顺序表明其对模型的价值:排序高的属性要比排序低的属性对模型准确度的贡献更大  
> >
> > 变量重要性:在特征工程中有助于对属性进行剪枝  
> > 
> 3. 部署时的预测足够快速(有时，快速计算预测结果是一个关键的性能参数)  
> > 
> > 不论哪种算法，其预测速度很难超越线性模型  
> >
> > 原因：线性模型在预测时，仅需要对包含的每个属性进行一次相乘以及一次相加操作  
> > 
> 4. 性能可靠  
>
> 稀疏解：
> > 
> > 模型中的许多系数等于 0
> > 
> > 预测时，相乘以及相加的次 数会减少
> > 
> > 稀疏模型（非0的系数较少）更容易解释（即更容易看到模型中 的哪些属性在驱动着预测结果）
> > 
> 5.问题本身可能需要线性模型 
>

* 4.2 惩罚线性回归(对线性回归进行正则化以获得最优性能)

> 一般方法是通过解最小化问题来找到属性乘子,最小化问题是找到使得均方误差最小(但不是 0)的β值  
>  
> ![avatar](/Blog-Share/img/1904/02/Doris/4.1.png)
>  
> 公式 4-4 的两边完全相等就意味着模型已经过拟合(右侧是要训练的预测模型)
>
> 子集选择通过丢弃一些属性来消除过拟合，实际等同于将这些属性的对应系数设为0。惩罚回归做同样的事情，但与子集选择直接将一些属性系数设为0不同，惩罚线性回归将每个属性系数都减少一些。
> 
> * 其他有用的系数惩罚项：Manhattan 以及 ElasticNet 
> > Manhattan:Lasso回归回归源于出租车的几何路径被称作曼哈顿距离或者L1正则化
> >
> > 岭回归以及Lasso回归的差异在于对 β（即线性系数向量）的惩罚上：
> > > 
> > > 岭回归使用欧式距离的平方，即β元素的平方和进行惩罚。Lasso回归使用β元素绝对值的加和
> > >  
> > (Lasso惩罚会导致稀疏的系数向量)
> > 
> > ElasticNet 惩罚项包含Lasso惩罚项以及岭惩罚项
> > (ElasticNet引入一个额外参数α用于控制岭惩罚项以及套索惩罚项的比例。α=1 表示只使用套索惩罚，不使用岭惩罚)
> > 
> > 在求解线性模型系数之前，λ 以及α 必须提前确定
> > 
* 4.3 求解惩罚线性回归问题  
>  LARS（最小角度回归）算法可以理解为一种改进的前向逐步回归算法  
> 对比LARS算法与前向逐步回归算法  
> > 前向逐步回归算法：  
> >  
> > ![avatar](/Blog-Share/img/1904/02/Doris/4.2.png)  
> >  
> > ![avatar](/Blog-Share/img/1904/02/Doris/4.3.png)
> >  
> > 差异：LARS 在引入新属性时只是部分引入，引入属性过程并非不可逆  
> >   
> 从数百个 LARS 生成结果中选择最佳模型：10 折交叉验证（将输入数据切分为 10 份几乎均等的数据，将其中一份数据移除，使用剩下数据进行训练，然后再在移除的数据上进行测试）  
> > 
> > 利用交叉验证进行模型选择
> > 
> > 在交叉验证的每一份数据上累加错误以及评估结果
> > 
> Glmnet算法(快速且通用 )
> > 
> > glmnet 算法生成了完整的系数曲线，类似于 LARS 算法
> > 

* 4.4 输入为数值型数据的线性回归方法的扩展(用多种方法将目前的回归问题推广到分类问题)

> 对于二分类问题，将二值转换为实数值（0和1，-1和1）
> 
> 求解超过 2 种输出的分类问题（多分类问题） 
> > 
> > 把多分类问题分解为多个二分类问题
> > 
> 理解基扩展：使用线性方法来解决非线性问题 
> > 
> > 基扩展的基本想法是问题中的非线性可以通过属性的多项式组合来近似（或者属性的其他非线性函数）
> > 可以向线性回归公式添加原始属性的幂作为回归因子，通过线性 方法来确定多项式回归的系数集合
> >
> 向线性方法中引入非数值属性 
> >
> >进行类别属性转换的标准方法是将属性的可能取值编码为若干新的属性列


