# 2019-04-17-简单理解支持向量机（SVM）.md

Author: 汤航

Date: 2019/04/17

## 引言

支持向量机在工程中有着广泛应用，虽然现在大家都是调调包就可以跑起SVM模型，但是理解其底层原理，有利于我们更顺利地使用它来工作或者对它进行改编扩展。

## SVM简介

SVM 是以统计学习理论为基础的一种机器学习算法，Vapnik 等人在 1992 — 1997 年建立了 SVM理论体系 ，包括支持向量分类机和支持向量回归机。 SVM 具有直观的几何解释、严格的数学理论和良好的泛化能力，尤其适合小样本学习问题；与人工神经网络算法相比， SVM 避免了神经网络中拓扑结构难以确定和局部量最优问题，并克服了“维数灾难”。因此， SVM 在许多工程问题中得到了广泛应用。

## SVM原理

### 一、决策边界
![图1](/Blog-Share/img/1904/02/thang/1.png)  
如上图所示，分类问题中，我们需要找决策边界来将样本点进行分类，如何找到最优的决策边界，SVM提供了它的方法。

### 二、SVM模型思想

![图2](/Blog-Share/img/1904/02/thang/2.png)  

如图2所示，对于 SVM 而言，它定义的准则是寻找一个离数据点最远的决策面（基于向量空间模型的机器学习方法其目标是找到两个类别之间的一个决策边界，使之尽量远离训练集上的任意一点）。从决策面到最近数据点的距离决定了分类器的间隔(margin)。这种思想意味着，SVM 的决策函数完全由部分的数据子集（通常的数量很小）确定，并且这些子集定义了分界面的位置。这些子集中的点称为**支持向量**。  

除支持向量外的其他数据点对最终分界面的确定不起作用。支持向量机就是寻找这样一个决策超平面，它可以使得两个类之间的分类间隔最大化。

### 三、SVM形式化描述

![图3](/Blog-Share/img/1904/02/thang/3.png)

如图3所示，决策超平面的数学描述为𝑊^𝑇 𝑥+𝑏=0。W为决策超平面的法向量，b为截距。在决策超平面H0上的所有点x，都满足𝑊^𝑇 𝑥+𝑏=0。假定训练集上的所有点为 D={(x i ,y i )},其中x i 是第 i 个样本数据，yi 是类别标签∈{+1,-1}。如此 SVM 的线性分类器可以表示成𝑓(𝑥)=𝑠𝑖𝑔𝑛(𝑊^𝑇 𝑥+𝑏)。这里sign函数是    
![公式1](/Blog-Share/img/1904/02/thang/t1.png)  
在图3所示的例子中，H1上的点就满足sign(𝑊^𝑇 𝑥+𝑏)=+1。在H2上的点sign(𝑊^𝑇 𝑥+𝑏)=-1。训练SVM分类器就是由训练集来求解W。

### 四、SVM数学模型推导

![图4](/Blog-Share/img/1904/02/thang/4.png)

如图4所示，一个点 x 到决策超平面的距离记为 r，由于点到超平面的最短距离垂直于该平面，也就是说和 w 平行。这个方向的单位向量是 w/|w|。图中的点线就是 rw/|w|的平移结果。将超平面上距离 x 最近的点标记为 x’,于是有 x’=x-yrw/|w|由于x’在决策边界上，因此有𝑊^𝑇x’+𝑏=0，于是𝑊^𝑇 (𝑥−𝑦𝑟𝑤/|𝑤|)+𝑏=0。  

前面提到，“支持向量机就是寻找这样一个决策超平面，它可以使得两个类之间的分类间隔最大化”。训练集中的每个点(𝑥_𝑖 , 𝑦_𝑖 )的类别标签𝑦_𝑖是+1 或-1。把该点带入支持向量机时，如果|𝑊^𝑇 𝑥_𝑖+b|<1 则不可以做正确分类。因此，SVM 的约束条件是𝑦_𝑖(𝑊^𝑇 𝑥_𝑖+b)>=1，且若 x 为支持向量， 𝑦_𝑖 (𝑊^𝑇 𝑥_𝑖+b)=1。因为 r=y(𝑊^𝑇x+b)/|w|，对于支持向量应满足 r=1/|w|。那么使得支向量的 r（分类间隔/2）最大时的|w|，就是我们需要求解的 w。即寻找 w 和 b 来满足分类间隔 ρ=2/|w| 极大化。标准的支持向量机公式是用最小化形式描述的：寻找 w 和 b 使得  
（1）𝑊^𝑇 𝑊/2极小化  
（2）对所有{(𝑥_𝑖, 𝑦_𝑖)}， 𝑦_𝑖(𝑊^𝑇 𝑥_𝑖+b)>=1  
  
我们将最终的目标函数和约束条件放在一起进行描述：得到我们将最终的目标函数和约束条件放在一起进行描述  
![公式2](/Blog-Share/img/1904/02/thang/t2.png)  
这里n是样本点的总个数，缩写s.t.表示”Subject to”，是”服从某某条件”的意思。上述公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。

### 五、求解最优解（一）  

接下来的问题就是如何根据数学模型，求得我们想要的最优解。  
这是一个不等式条件下的二次优化问题，可以使用拉格朗日函数方法来解决，需要进行下面两个步骤：  
  1.将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数  
  2.使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化  
  
下面，进行第一步：将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数  
公式变形如下：  

![公式3](/Blog-Share/img/1904/02/thang/t3.png)  
  
其中αi是拉格朗日乘子，αi大于等于0，是我们构造新目标函数时引入的系数变量(我们自己设置)。现在我们令：  
![公式4](/Blog-Share/img/1904/02/thang/t4.png)  
  
这里的d表示这个问题的最优值，且和最初的问题是等价的。  
  
首先求内部最小值，对L(w,b,α)关于w和b最小化，我们分别对w和b偏导数，令其等于0，即：  
![公式5](/Blog-Share/img/1904/02/thang/t5.png)  
  
将上述结果带回L(w,b,α)得到：  
![公式6](/Blog-Share/img/1904/02/thang/t6.png)  
  
现在内测最小值求到了，现在求外部最大值：  
![公式7](/Blog-Share/img/1904/02/thang/t7.png)  

### 六、求解最优解（二）

之所以要将优化问题变成这个形式，是为了便于使用SMO算法来解。  
SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面。  
SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到了一对合适的alpha，那么就增大其中一个同时减小另一个。  
通过不断迭代更新alpha和b最终求解出满足条件的alpha，最后再由alpha带回求导式，算出原求解问题的最优W。  
  

### 七、非线性SVM与松弛变量  

前面我们已经了解到SVM如何处理线性可分的情况，而对于非线性的情况，可以将样本点映射到更高维的空间中，就可以转化为线性可分来处理。  
SVM对非线性情况的处理方式是选择一个核函数，来达到映射到高维空间的效果。常用的径向基核函数是SVM中常用的一个核函数。径向基核函数采用向量作为自变量的函数，能够基于向量举例运算输出一个标量。径向基核函数的高斯版本的公式如下：  
![公式8](/Blog-Share/img/1904/02/thang/t8.png)  
  
另外，当样本中存在噪声时，会出现分类错误，可以引入正则化变量C和松弛变量和ξ_i，一个非零的ξ_i表示允许x_i在未满足间隔需求下的惩罚量或代价因子（或者说 ξ_i 是对应数据点 x_i 允许偏离的 margin 的量）。    

## 参考文章
* [手撕线性SVM](https://blog.csdn.net/c406495762/article/details/78072313)
* [手撕非线性SVM](https://blog.csdn.net/c406495762/article/details/78158354)
